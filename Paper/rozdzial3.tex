\section{Metodologia i implementacja rozwiązania}
\label{sec:metodologia}

Poprzednie rozdziały dokonały teoretycznej dekonstrukcji technologii kamer IP (Rozdział~1) oraz przeprowadziły szczegółową analizę studium przypadku — kamery TP-Link Tapo C200 (Rozdział~2). Analiza ta zidentyfikowała kluczowy problem badawczy: fundamentalny konflikt między potencjałem sprzętowym urządzenia a ograniczeniami narzuconymi przez zamknięty ekosystem producenta (tzw. \textbf{„vendor lock-in”}).

Niniejszy rozdział przechodzi od teorii do praktyki. Stanowi on techniczną odpowiedź na zdefiniowane wyzwania. Opisany zostanie kompletny proces projektowy i wdrożeniowy – od wybranej metodyki badawczej, przez architekturę systemu, aż po szczegóły implementacji poszczególnych komponentów. Celem jest budowa autorskiego, otwartego rozwiązania programistycznego, które uwalnia pełen potencjał kamery i realizuje cele postawione w niniejszej pracy.

\subsection{Metodyka Projektowa}
\label{subsec:metodyka_projektowa}

\subsubsection{Double Diamond}
\label{subsubsec:double_diamond}

Model Double Diamond (Podwójny Diament) jest ustrukturyzowaną metodyką procesową, pierwotnie sformalizowaną przez British Design Council w 2005 roku. Stanowi ona mapę procesu projektowego, którego celem jest efektywne nawigowanie od wstępnej idei do wdrożonego rozwiązania, przy jednoczesnym zarządzaniu złożonością i niepewnością.

Metodyka ta jest fundamentalna dla współczesnego projektowania (w tym inżynierii oprogramowania, projektowania produktów i usług) i bazuje na koncepcji myślenia projektowego (Design Thinking).

Nazwa modelu pochodzi od jego wizualnej reprezentacji jako dwóch sąsiadujących rombów („diamentów”). Każdy diament reprezentuje sekwencję dwóch typów myślenia:

\begin{itemize}
    \item \textbf{Myślenie Rozbieżne:} Faza otwierania „diamentu”. Polega na eksploracji, generowaniu dużej liczby pomysłów, zbieraniu szerokiego spektrum danych i powstrzymywaniu się od oceny. Celem jest poszerzenie perspektywy i zrozumienie kontekstu.
    
    \item \textbf{Myślenie Zbieżne:} Faza zamykania „diamentu”. Polega na syntezie, analizie, krytycznej ocenie i podejmowaniu decyzji. Celem jest zawężenie opcji i wyłonienie konkretnego kierunku działania.
\end{itemize}

Model zakłada, że aby opracować właściwe rozwiązanie, należy najpierw dogłębnie zrozumieć i zdefiniować właściwy problem.

\subsubsection*{Diament 1: Przestrzeń Problemu}
Celem tego etapu jest zidentyfikowanie i precyzyjne zdefiniowanie kluczowego problemu, który ma zostać rozwiązany.

\begin{enumerate}
    \item \textbf{Faza Odkrywania (Discover) – Dywergencja} Jest to faza intensywnych badań (research) i empatii. Zespół projektowy wychodzi poza własne założenia, aby zrozumieć rzeczywisty kontekst użytkownika i zidentyfikować jego niezaspokojone potrzeby.
    
    \item \textbf{Faza Definiowania (Define) – Konwergencja} W tej fazie następuje synteza danych zebranych podczas Odkrywania. Zespół filtruje i analizuje informacje, szukając wzorców i kluczowych wyzwań. Celem jest przekształcenie rozproszonych obserwacji w klarowną i mierzalną definicję problemu.
\end{enumerate}

\subsubsection*{Diament 2: Przestrzeń Rozwiązania}

\begin{enumerate}
    \setcounter{enumi}{2} % Kontynuacja numeracji od 3
    \item \textbf{Faza Rozwijania (Develop) – Dywergencja} Mając jasno zdefiniowany problem, zespół ponownie przechodzi w tryb dywergencyjny, aby wygenerować jak najszerszy wachlarz potencjalnych rozwiązań. Kładzie się nacisk na ilość, a nie jakość, oraz na kreatywność i multidyscyplinarność.
    
    \item \textbf{Faza Dostarczania (Deliver) – Konwergencja} Jest to ostatnia faza, skupiona na testowaniu, walidacji i iteracyjnym udoskonalaniu wybranych koncepcji. Rozwiązania są poddawane rygorystycznym testom z udziałem użytkowników, aby zidentyfikować błędy i obszary do poprawy, a następnie zawęzić wybór do jednego, optymalnego rozwiązania gotowego do wdrożenia.
\end{enumerate}

\subsubsection*{Zastosowanie modelu w niniejszej pracy}

\begin{enumerate}
    \item \textbf{Odkrywanie (Discover):} Tę fazę reprezentuje research przeprowadzony w Rozdziałach 1 i 2. Zbadano ogólne działanie kamer IP, a następnie przeanalizowano specyfikę Tapo C200, identyfikując jej otwarte porty (RTSP) oraz zamknięte, własnościowe API do sterowania PTZ.
    
    \item \textbf{Definiowanie (Define):} Zebrane informacje skumulowano do konkretnego problemu: \textbf{vendor lock-in} uniemożliwia lokalną kontrolę. Celem pracy stało się więc stworzenie lokalnego systemu dającego pełną kontrolę.
    
    \item \textbf{Rozwój (Develop):} W tej fazie nastąpił brainstorming nad architekturą rozwiązania. Rozważano różne technologie i narzędzia (np. gotowe platformy vs. własna aplikacja). Zdecydowano się na elastyczny stos technologiczny, który umożliwi realizację wszystkich celów.
    
    \item \textbf{Dostarczanie (Deliver):} Wybrano konkretne, optymalne rozwiązanie: aplikacja webowa oparta na Pythonie, Flasku i WebSockets, wykorzystująca bibliotekę PyTapo (do sterowania) oraz OpenCV i FFmpeg (do analizy wideo), całość hermetyzowana w Dockerze. Implementacja tego rozwiązania stanowi dalszą część niniejszego rozdziału.
\end{enumerate}




\subsection{Architektura rozwiązania}
\label{subsec:architektura}

System został zaprojektowany jako \textbf{Real-Time IoT Gateway}, stanowiąca pomost między klientami internetowymi o wysokim opoznien a niskopoziomowymi protokołami sprzętowymi. U swej podstawy aplikacja opiera się na \textbf{Architekturze Trójwarstwowej} (\textit{Three-Tier Multitier Architecture}), ściśle oddzielając Warstwę Prezentacji (Klient), Logikę Aplikacji (Middleware) oraz Warstwę Danych/Sprzętową (Źródło). Taka separacja zapewnia, że złożoność własnościowych protokołów kamery zostaje całkowicie abstrahowana i ukryta przed interfejsem użytkownika końcowego.

Kluczową decyzją projektową była implementacja aplikacji w języku Python jako inteligentnej \textbf{warstwy pośredniej} (\textit{Middleware}). Ponieważ nowoczesne przeglądarki internetowe nie są w stanie natywnie obsługiwać surowych strumieni wideo RTSP ani komunikować się bezpośrednio za pomocą protokołu ONVIF, warstwa pośrednia działa jako dwukierunkowy translator protokołów. Pobiera ona synchroniczne strumienie sprzętowe i przekształca je w asynchroniczne zdarzenia WebSocket. Pozwala to na uzyskanie responsywnego doświadczenia użytkownika bez konieczności ujawniania danych uwierzytelniających sprzętu czy jego adresu IP w sieci publicznej.

W celu sprostania specyficznym wymaganiom przetwarzania audiowizualnego, system wykorzystuje hybrydę 2 wzorców architektonicznych. Pierwszym z nich jest wzorzec 
\textbf{Architektury Potokowej} (\textit{ ang. Pipe and Filter}). Zastosowany w warstwie przetwarzania do sekwencyjnej obsługi klatek wideo i fragmentów audio (Akwizycja $\rightarrow$ Przetwarzanie $\rightarrow$ Kodowanie $\rightarrow$ Emisja).

Kolejnym wzorcem  architektonicznym jest \textbf{Architektura Sterowana Zdarzeniami} (\textit{Event-Driven Architecture}), która umożliwia komunikację w czasie rzeczywistym między klientem a serwerem. Akcje użytkownika (takie jak sterowanie PTZ) oraz zmiany stanu systemu (np. wykrycie ruchu) są propagowane natychmiastowo poprzez magistralę zdarzeń (WebSockets), zamiast polegać na cyklicznym odpytywaniu (\textit{polling}).


Ze względu na ciągły charakter strumieniowania wideo, architektura systemu w znacznym stopniu polega na \textbf{wielowątkowości} (\textit{threaded concurrency}). Aplikacja utrzymuje współdzielony stan w pamięci operacyjnej (\textit{shared in-memory state}), co pozwala na odseparowanie szybkich pętli wejściowych (odczyt ze sprzętu) od obsługi żądań wyjściowych (obsługa klientów webowych). Gwarantuje to, że obciążające procesor zadania, takie jak detekcja ruchu czy muksowanie wideo, nie blokują interfejsu użytkownika.

W kolejnych sekcjach szczegółowo omówiono odpowiedzialności poszczególnych warstw (Prezentacji, Logiki i Danych), przeanalizowano wewnętrzny przepływ danych w potokach medialnych oraz przedstawiono strukturę klas wykorzystaną do implementacji powyższej architektury.

% Miejsce na diagram architektury (jeśli posiadasz plik graficzny)
\begin{figure}[h]
    \centering
    % \includegraphics[width=0.9\textwidth]{architektura_diagram.png}
    \caption{Schemat architektury rozwiązania (Klient - Middleware - Sprzęt)}
    \label{fig:architektura_diagram}
\end{figure}

\subsubsection{Architektura Wielowarstwowa}
\label{subsubsec:architektura_wielowarstwowa}

Współczesna inżynieria systemów \textbf{Internetu Rzeczy} (\textit{IoT}), a w szczególności projektowanie bram sieciowych (\textit{IoT Gateways}) obsługujących strumieniowanie multimediów w czasie rzeczywistym, wymaga rygorystycznego podejścia do strukturalizacji kodu oraz zarządzania przepływem danych. W ramach niniejszej pracy inżynierskiej, jako fundament logiczny i fizyczny rozwiązania, przyjęto \textbf{Architekturę Trójwarstwową}.

Architektura warstwowa jest powszechnie uznawana w literaturze przedmiotu za \textit{de facto} standard w projektowaniu aplikacji korporacyjnych i systemów rozproszonych, umożliwiając dekompozycję złożonego problemu na separowalne, zarządzalne poziomy abstrakcji. W kontekście systemów IoT, model ten ewoluuje w kierunku struktur typu \textbf{Edge-Fog-Cloud}, gdzie brama (\textit{Gateway}) pełni rolę kluczowego węzła pośredniczącego. Zastosowany w projekcie model trójwarstwowy dokonuje ścisłej separacji odpowiedzialności (\textit{Separation of Concerns - SoC}) pomiędzy interakcją z użytkownikiem, logiką biznesową przetwarzania sygnału oraz fizycznym dostępem do urządzenia.

Poniższa tabela (Tabela \ref{tab:warstwy_architektury}) przedstawia szczegółowy podział odpowiedzialności oraz stos technologiczny wykorzystany w poszczególnych warstwach systemu.

\begin{table}[h]
    \centering
    \small % Zmniejszenie czcionki dla lepszej czytelności dużej tabeli
    \renewcommand{\arraystretch}{1.5} % Zwiększenie odstępów między wierszami
    \caption{Podział warstw architektury systemu IoT}
    \label{tab:warstwy_architektury}
    
    % Tabela rozciągnięta na całą szerokość tekstu
    \begin{tabularx}{\textwidth}{@{} p{3cm} L L L @{}}
        \toprule
        \textbf{Poziom Architektury (Tier)} & 
        \textbf{Rola w Systemie IoT} & 
        \textbf{Implementacja (Stack Technologiczny)} & 
        \textbf{Odpowiedzialność Funkcjonalna} \\
        \midrule
        
        \textbf{Tier 1:} \newline Warstwa Prezentacji (\textit{Presentation Layer}) & 
        Interfejs Użytkownika (GUI), wizualizacja danych, obsługa zdarzeń wejściowych. & 
        \textbf{Klient Webowy (SPA):} HTML5, JavaScript, Socket.IO Client, HTML5 Canvas. & 
        Renderowanie strumienia wideo (MJPEG/Canvas), panel sterowania PTZ, wyświetlanie alertów detekcji ruchu. \\
        \midrule
        
        \textbf{Tier 2:} \newline Warstwa Logiki (\textit{Business Logic / Middleware}) & 
        Przetwarzanie reguł, koordynacja procesów, analiza danych, translacja protokołów. & 
        \textbf{Serwer Aplikacyjny:} Python 3.13, Flask, Flask-SocketIO, OpenCV (\textit{Computer Vision}), Multiprocessing. & 
        Detekcja ruchu (\textit{Background Subtraction}), obsługa sesji WebSocket, buforowanie klatek, orkiestracja wątków. \\
        \midrule
        
        \textbf{Tier 3:} \newline Warstwa Danych i Sprzętu (\textit{Data/Hardware Layer}) & 
        Fizyczny dostęp do danych, abstrakcja sprzętowa, trwała pamięć masowa. & 
        \textbf{HAL \& DAO:} PyTapo (Driver), FFmpeg (Video Capture), System Plików (Storage). & 
        Komunikacja z API Tapo, obsługa strumienia RTSP, zapis nagrań na dysk, zarządzanie poświadczeniami. \\
        \bottomrule
    \end{tabularx}
\end{table}

W dalszej części rozdziału przeprowadzona zostanie szczegółowa analiza każdej z warstw, z naciskiem na uzasadnienie doboru technologii oraz analizę wydajnościową przyjętych rozwiązań.


\paragraph{Warstwa Prezentacji}
\label{par:warstwa_prezentacji}

Warstwa Prezentacji stanowi najwyższy poziom abstrakcji w systemie, będąc jedynym punktem styku użytkownika z infrastrukturą monitoringu. Zaproponowane rozwiązanie opiera się na modelu \textbf{cienkiego klienta} (\textit{thin client}). Podejście to zakłada, że przeglądarka internetowa odpowiada wyłącznie za renderowanie obrazu i przesyłanie zdarzeń sterujących, podczas gdy ciężar obliczeniowy związany z dekodowaniem, analizą wizyjną i zarządzaniem bezpieczeństwem spoczywa na serwerze.

Wybór architektury \textit{thin client} dla warstwy prezentacji jest podyktowany specyfiką środowiska IoT oraz dążeniem do uniwersalności. Klienci ,,ciency'' charakteryzują się mniejszymi wymaganiami sprzętowymi po stronie użytkownika, co umożliwia dostęp do systemu monitoringu z szerokiego spektrum urządzeń – od wydajnych stacji roboczych, po budżetowe smartfony i tablety, bez konieczności instalacji dedykowanego oprogramowania. Co więcej, centralizacja logiki na serwerze ułatwia zarządzanie bezpieczeństwem – wrażliwe algorytmy detekcji oraz bezpośrednie poświadczenia do kamery nigdy nie opuszczają bezpiecznej strefy serwera, co drastycznie redukuje wektor ataku.

W omawianym rozwiązaniu Warstwa Prezentacji została zaimplementowana jako responsywna aplikacja internetowa, wykorzystująca standardowe technologie webowe (HTML5, CSS3, JavaScript), co eliminuje konieczność instalacji dodatkowych wtyczek po stronie klienta. Architektura interfejsu została podzielona na dwa funkcjonalne moduły, różniące się sposobem obsługi mediów:

\begin{enumerate}
    \item \textbf{Moduł czasu rzeczywistego (Live Dashboard)}
    
    W przypadku podglądu na żywo zrezygnowano z tradycyjnego podejścia opartego na znaczniku wideo HTML5 (\texttt{<video>}), który ze względu na wewnętrzne mechanizmy buforowania przeglądarki wprowadza opóźnienia nieakceptowalne przy sterowaniu kamerą. Zamiast tego zastosowano renderowanie imperatywne na elemencie \textbf{Canvas}. Mechanizm ten działa w ścisłej symbiozie z warstwą logiczną serwera:
    
    \begin{description}
        \item[Wizualizacja:] Przeglądarka traktowana jest jako dynamiczne płótno, na którym rysowane są poszczególne ramki obrazu otrzymywane asynchronicznie poprzez kanał \textbf{WebSocket}. Pozwala to na zachowanie płynności i minimalizację opóźnień (\textit{low-latency}), gdyż pomijany jest narzut związany z konteneryzacją strumienia wideo.
        
        \item[Interakcja:] Sterowanie mechaniką kamery (PTZ) oraz procesem nagrywania zrealizowano w oparciu o model zdarzeniowy. Akcje użytkownika nie powodują przeładowania strony, lecz generują natychmiastowe sygnały sterujące wysyłane do serwera.
        
        \item[Audio:] Warstwa Prezentacji przejmuje również odpowiedzialność za rekonstrukcję dźwięku. Surowe dane audio są buforowane i odtwarzane przy użyciu \textbf{Web Audio API}, co pozwala na synchronizację ścieżki dźwiękowej z obrazem bez obciążania łącza przesyłaniem ciężkich plików multimedialnych.
    \end{description}

    \item \textbf{Moduł archiwizacji}
    
    Dla funkcjonalności przeglądania nagrań historycznych zastosowano odmienną strategię. Ponieważ w tym przypadku priorytetem jest stabilność odtwarzania i możliwość przewijania materiału, a nie czas reakcji, warstwa prezentacji wykorzystuje natywne możliwości przeglądarki do odtwarzania plików wideo (MP4) serwowanych standardowym protokołem HTTP.
\end{enumerate}

Całość interfejsu została zaprojektowana zgodnie z zasadami \textbf{Responsive Web Design} (RWD), co zapewnia spójne doświadczenie użytkownika niezależnie od tego, czy system obsługiwany jest na monitorze stacji roboczej, czy na ekranie dotykowym urządzenia mobilnego. Rolę medium transportowego dla danych dynamicznych pełni tutaj biblioteka klienta WebSocket, która zarządza stabilnością połączenia z warstwą pośrednią (\textit{Middleware}).

\paragraph{Warstwa Logiki}
\label{par:warstwa_logiki}

Warstwa Logiki, umiejscowiona centralnie w architekturze trójwarstwowej, pełni rolę ,,mózgu'' systemu, orkiestrując przepływ danych pomiędzy użytkownikiem a sprzętem. W literaturze dotyczącej IoT warstwa ta jest często definiowana jako \textit{Middleware}, którego zadaniem jest ukrycie heterogeniczności urządzeń końcowych i udostępnienie ujednoliconych usług dla warstwy aplikacji. W niniejszym projekcie warstwa ta została zaimplementowana w języku \textbf{Python 3.13} z wykorzystaniem frameworka \textbf{Flask}, działając jako inteligentna brama (\textit{Smart Gateway}) realizująca nie tylko routing, ale i zaawansowane przetwarzanie danych na brzegu sieci (\textit{Edge Computing}).

\subparagraph{Middleware jako Translator Protokołów i Agregator}

Głównym zadaniem Warstwy Logiki jest integracja różnorodnych standardów komunikacyjnych. Kamera Tapo C200 operuje na dwóch niezależnych kanałach: standardowym strumieniu wideo RTSP (port 554) oraz zamkniętym, szyfrowanym protokole sterowania HTTP (\textit{proprietary API}). Warstwa Logiki działa tutaj jako \textbf{Translator Protokołów}, konwertując te niespójne interfejsy na jednolity strumień zdarzeń WebSocket zrozumiałych dla klienta webowego. \textit{Middleware} realizuje następujące funkcje kluczowe:

\begin{description}
    \item[Ingestia i Transkodowanie:] Pobiera strumień H.264 z kamery, dekoduje go i transkoduje do formatu MJPEG/JPEG w czasie rzeczywistym, co jest niezbędne, gdyż przeglądarki nie obsługują natywnie surowego RTSP.
    
    \item[Abstrakcja Sterowania:] Tłumaczy wysokopoziomowe polecenia (np. \texttt{move\_camera('left')}) na specyficzne, zaszyfrowane payloady wymagane przez API Tapo, wykorzystując biblioteki warstwy niższej.
    
    \item[Zarządzanie Sesjami:] Monitoruje stan połączeń klientów. Dzięki architekturze stanowej (\textit{Stateful}), system może wstrzymać pobieranie strumienia z kamery, gdy żaden użytkownik nie jest podłączony, co znacząco oszczędza zasoby sieciowe i obliczeniowe bramy.
\end{description}

\subparagraph{Wyzwanie Współbieżności: Python GIL i Model Przetwarzania}

Krytycznym aspektem projektowym w Warstwie Logiki był wybór modelu współbieżności. Język Python, mimo swojej elastyczności i bogatego ekosystemu bibliotek (\texttt{OpenCV}, \texttt{NumPy}), posiada istotne ograniczenie w postaci \textbf{Global Interpreter Lock} (GIL). GIL to mechanizm w referencyjnej implementacji CPython, który wymusza, aby w danym momencie tylko jeden wątek wykonywał kod bajtowy Pythona. W systemie przetwarzania wideo, który musi jednocześnie odbierać dane z sieci (I/O), analizować obraz (CPU) i wysyłać go do klientów (I/O), GIL staje się poważnym wąskim gardłem, uniemożliwiającym pełne wykorzystanie procesorów wielordzeniowych w modelu wielowątkowym.

Aby rozwiązać ten problem i zapewnić wysoką wydajność (wysoki FPS, niskie opóźnienia), w projekcie zastosowano hybrydowy model współbieżności, oparty na analizie charakterystyki zadań:

\begin{description}
    \item[Wielowątkowość (\texttt{threading}) dla zadań I/O-bound:] Obsługa serwera WWW, gniazd WebSocket oraz komunikacji sieciowej z kamerą jest realizowana za pomocą wątków (przy użyciu biblioteki \texttt{eventlet} lub \texttt{gevent} zintegrowanej z Flask-SocketIO). Operacje wejścia/wyjścia w Pythonie zwalniają blokadę GIL, co pozwala na efektywną obsługę wielu równoległych połączeń sieciowych w jednym procesie.
    
    \item[Wieloprocesowość (\texttt{multiprocessing}) dla zadań CPU-bound:] Kluczowe zadania obliczeniowe, takie jak dekodowanie klatek wideo i algorytmy detekcji ruchu (\textit{Computer Vision}), zostały wydzielone do osobnych procesów systemowych. Każdy proces posiada własną instancję interpretera Python i własny GIL, co pozwala na rzeczywiste równoległe wykonywanie kodu na dostępnych rdzeniach procesora (np. 4 rdzenie w Raspberry Pi). Badania wskazują, że dla zadań przetwarzania obrazu w \texttt{OpenCV}, model wieloprocesowy oferuje liniowy wzrost wydajności, podczas gdy model wątkowy jest ograniczony przez GIL i nie skaluje się.
\end{description}

\subparagraph{Implementacja Wzorca Producent-Konsument}

Przepływ danych wideo wewnątrz Warstwy Logiki został zorganizowany zgodnie z asynchronicznym wzorcem projektowym \textbf{Producent-Konsument} (\textit{Producer-Consumer Pattern}), co zapewnia separację procesu akwizycji danych od ich dystrybucji.

\begin{description}
    \item[Proces Producenta:] Dedykowany proces, który w pętli nieskończonej pobiera klatki ze strumienia RTSP (używając \texttt{cv2.VideoCapture}). Jego wyłącznym zadaniem jest utrzymanie stabilnego połączenia z kamerą i umieszczanie najnowszej klatki w pamięci współdzielonej (kolejce).
    
    \item[Bufor (Kolejka):] Zastosowano kolejkę o bardzo małym rozmiarze (np. \texttt{maxsize=1}). Jest to celowy zabieg inżynierski wynikający ze specyfiki \textit{live streamingu}: w systemie monitoringu ważniejsza jest minimalizacja opóźnienia (\textit{latency}) niż zachowanie każdej klatki. Jeśli proces konsumenta (serwer wysyłający) jest zbyt wolny, kolejka automatycznie odrzuca stare klatki, zapewniając, że klient zawsze otrzymuje najbardziej aktualny obraz (,,gubienie klatek'' zamiast ,,buforowania opóźnień'').
    
    \item[Proces Konsumenta:] Wątek serwera Flask, który pobiera klatkę z kolejki, aplikuje na nią logikę biznesową (np. detekcję ruchu) i emituje do klientów WebSocket.
\end{description}

Taka architektura zapewnia izolację błędów – awaria lub spowolnienie po stronie klienta webowego nie wpływa na stabilność połączenia z kamerą, a chwilowe problemy z siecią kamery nie blokują interfejsu użytkownika.


\paragraph{Warstwa Danych}
\label{par:warstwa_danych}

Najniższy poziom architektury stanowi fundament integrujący system cyfrowy ze światem fizycznym. W klasycznej inżynierii oprogramowania warstwa ta (\textit{Data Access Layer} - DAL) odpowiada za komunikację z bazą danych. W systemach IoT pojęcie to ulega rozszerzeniu o \textbf{Warstwę Abstrakcji Sprzętu} (\textit{Hardware Abstraction Layer} - HAL). W projekcie przyjęto założenie, że kamera IP jest specyficznym rodzajem ,,bazy danych'', która dostarcza strumienie informacji (wideo, audio, telemetria) i przyjmuje polecenia modyfikacji stanu (PTZ, konfiguracja).

\subparagraph{Warstwa Abstrakcji Sprzętu (HAL) jako Izolator}

Podstawowym celem implementacji HAL jest uniezależnienie wyższych warstw systemu od konkretnego modelu sprzętowego. Warstwa Logiki nie powinna operować na niskopoziomowych szczegółach, takich jak adresy URL strumieni RTSP, algorytmy szyfrowania haseł czy specyficzne kody błędów HTTP zwracane przez kamerę. Zamiast tego, HAL udostępnia ujednolicony interfejs programistyczny (API wewnętrzne), np. metodę \texttt{camera.move\_left()}, która ,,pod spodem'' wykonuje całą komunikacyjną ,,brudną robotę''.

W projekcie HAL realizowany jest poprzez wzorzec \textbf{Adapter}, który ,,opakowuje'' zewnętrzną bibliotekę \texttt{PyTapo}. \texttt{PyTapo} jest efektem inżynierii wstecznej społeczności \textit{Open Source} i służy do komunikacji z zamkniętym API Tapo. Bezpośrednie użycie \texttt{PyTapo} w kontrolerach frameworka Flask naruszyłoby zasadę separacji odpowiedzialności. Stworzenie własnej klasy typu \textit{wrapper} (\texttt{TapoCameraDriver}) w warstwie HAL pozwala na:

\begin{description}
    \item[Łatwą wymianę sterownika:] Jeśli w przyszłości biblioteka \texttt{PyTapo} przestanie być rozwijana, wystarczy podmienić implementację wewnątrz klasy \texttt{TapoCameraDriver} na inną, bez konieczności przepisywania setek linii kodu w Warstwie Logiki.
    
    \item[Centralizację obsługi błędów:] HAL tłumaczy specyficzne wyjątki sieciowe (np. \texttt{ConnectionRefusedError} czy kody błędów z serwera \texttt{uhttpd} kamery) na zrozumiałe wyjątki domenowe (np. \texttt{CameraOfflineException}), upraszczając logikę obsługi błędów w wyższych warstwach.
    
    \item[Bezpieczeństwo:] HAL odpowiada za bezpieczne przechowywanie i wstrzykiwanie poświadczeń (login/hasło) do żądań. Dzięki temu dane uwierzytelniające nigdy nie ,,wyciekają'' do warstwy prezentacji. Jest to kluczowe w kontekście znanych podatności kamer Tapo, takich jak \texttt{CVE-2021-4045} (luka RCE w serwerze \texttt{uhttpd}), która wymusza traktowanie urządzenia jako potencjalnie niebezpiecznego i izolowanie interakcji z nim.
\end{description}
\subsubsection{Wzorzec Architektury Potokowej}
\label{subsubsec:architektura_potokowa}

Uzupełnieniem struktury warstwowej w warstwie logiki biznesowej jest zastosowanie \textbf{Architektury Potokowej} (ang. \textit{Pipe and Filter}). Wzorzec ten jest standardem w systemach przetwarzających strumienie danych multimedialnych, gdzie kluczowe jest zachowanie ciągłości i niskiego opóźnienia przetwarzania.

\paragraph{Zasada działania}

Istotą tego wzorca jest dekompozycja złożonego procesu przetwarzania danych na serię niezależnych, sekwencyjnych kroków (filtrów), połączonych kanałami komunikacyjnymi (potokami). Dane wejściowe wchodzą do systemu, przechodzą przez szereg transformacji, a następnie są przekazywane do wyjścia (ujścia). Kluczową cechą tej architektury jest izolacja poszczególnych filtrów – każdy z nich wykonuje tylko jedną, specyficzną operację (np. skalowanie obrazu, detekcja ruchu) i przekazuje zmodyfikowane dane do następnego elementu w łańcuchu. Takie podejście sprzyja modularności, ułatwia testowanie oraz pozwala na łatwą wymianę algorytmów bez naruszania struktury całego systemu.

\paragraph{Zastosowanie w projekcie}

W zrealizowanym systemie wzorzec ten stanowi fundament działania klas \texttt{VideoStreamer} oraz \texttt{AudioStreamer}, które operują w nieskończonych pętlach wątków tła. Ponieważ dane z kamery (protokół RTSP) napływają w sposób ciągły, każda jednostka danych (klatka wideo lub pakiet audio) musi zostać przetworzona w czasie rzeczywistym, zanim zostanie nadpisana przez kolejną. Architektura potokowa zapewnia tutaj deterministyczny przepływ danych od momentu ich akwizycji ze sprzętu aż do momentu wysłania do klienta webowego lub zapisu na dysku.

Wzorzec ten został zaimplementowany w następujących obszarach systemu:

\begin{itemize}
    \item \textbf{Potok Wideo:} Przekształcanie surowych macierzy pikseli w obrazy JPEG wyświetlane w przeglądarce.
    \item \textbf{Potok Audio:} Dekodowanie, resampling i miksowanie kanałów dźwiękowych.
    \item \textbf{Potok Rejestracji:} Buforowanie ramek w pamięci RAM i ich finalna kompozycja do pliku MP4 (realizowana przez bibliotekę \texttt{MoviePy}).
\end{itemize}

\paragraph{Przykład implementacji: Potok przetwarzania wideo}

Najbardziej reprezentatywnym przykładem wykorzystania tego wzorca w projekcie jest pętla przetwarzania obrazu zaimplementowana w klasie \texttt{VideoStreamer}. Proces ten można przedstawić jako sekwencję pięciu filtrów:

\begin{enumerate}
    \item \textbf{Źródło (\textit{Source}):} Metoda \texttt{camera.read\_frame()} dokonuje akwizycji surowej klatki obrazu bezpośrednio ze sterownika sprzętowego.
    
    \item \textbf{Filtr Optymalizacyjny:} Surowy obraz, często o wysokiej rozdzielczości natywnej, jest poddawany operacji skalowania (\texttt{cv2.resize}). Zmniejszenie rozdzielczości na tym etapie jest krytyczne dla wydajności kolejnych kroków analizy i transmisji.
    
    \item \textbf{Filtr Analityczny (\textit{Motion Detection}):} Przeskalowana klatka trafia do modułu \texttt{MotionDetector}. Jest ona porównywana z modelem tła (średnią kroczącą z poprzednich klatek). Wynikiem tego filtra nie jest modyfikacja obrazu, lecz wygenerowanie metadanych (flaga \texttt{is\_motion}), które sterują logiką powiadomień.
    
    \item \textbf{Filtr Kodujący:} Obraz będący macierzą pikseli (format BGR) jest kompresowany do formatu JPEG (\texttt{cv2.imencode}). Jest to niezbędny krok transformacji danych do formatu zrozumiałego dla przeglądarek internetowych.
    
    \item \textbf{Ujście (\textit{Sink}):} Zakodowany obraz, wraz z metadanymi o detekcji ruchu, jest przekazywany do warstwy transportowej (\texttt{socketio.emit}), która dystrybuuje go do wszystkich podłączonych klientów.
\end{enumerate}

Dzięki zastosowaniu architektury potokowej, dodanie nowej funkcjonalności – np. rozpoznawania twarzy – sprowadzałoby się jedynie do wpięcia nowego ,,filtra'' pomiędzy etap skalowania a kodowania, bez konieczności modyfikacji logiki pobierania obrazu czy komunikacji sieciowej.


\subsubsection{Wzorzec Architektury Opartej na Zdarzeniach}
\label{subsubsec:architektura_zdarzen}

Trzecim filarem architektonicznym omawianego systemu, odpowiedzialnym za interaktywność i komunikację między warstwami, jest \textbf{Architektura Oparta na Zdarzeniach} (ang. \textit{Event-Driven Architecture} – EDA). W przeciwieństwie do klasycznego modelu żądanie-odpowiedź (\textit{Request-Response}), typowego dla statycznych stron WWW, model ten zakłada, że przepływ sterowania w systemie jest determinowany przez wystąpienie określonych zdarzeń (akcji użytkownika, zmian stanu czujników), a nie przez sekwencyjny kod proceduralny.

\paragraph{Zasada działania}

Istotą EDA jest odwrócenie zależności komunikacyjnych. Komponenty systemu nie odpytują się wzajemnie o zmianę stanu (co generowałoby zbędny ruch sieciowy i opóźnienia), lecz oczekują na nadejście sygnału. Wzorzec ten składa się z trzech głównych elementów:

\begin{description}
    \item[Producent Zdarzenia (\textit{Event Producer}):] Komponent, który wykrywa zmianę (np. naciśnięcie przycisku, wykrycie ruchu) i emituje komunikat. Producent nie musi wiedzieć, kto i w jaki sposób obsłuży to zdarzenie.
    
    \item[Kanał Zdarzeń (\textit{Event Channel}):] Medium transportowe, które przekazuje zdarzenie od producenta do konsumenta. W projekcie rolę tę pełni biblioteka \texttt{Flask-SocketIO} działająca na protokole \textbf{WebSocket}.
    
    \item[Konsument Zdarzenia (\textit{Event Consumer}):] Komponent, który nasłuchuje na określony typ zdarzenia i w reakcji na nie uruchamia odpowiednią logikę biznesową.
\end{description}

\paragraph{Zastosowanie w projekcie}

W zrealizowanym systemie bramy IoT, architektura sterowana zdarzeniami została wykorzystana jako główny mechanizm komunikacji dwukierunkowej (\textit{Full-Duplex}) między Warstwą Prezentacji (przeglądarką) a Warstwą Logiki (serwerem Python). Zastosowanie tego wzorca było niezbędne do osiągnięcia niskiej latencji (opóźnienia) wymaganej przy zdalnym sterowaniu mechanicznym oraz do natychmiastowego powiadamiania użytkownika o zagrożeniach.
Wzorzec ten obsługuje trzy kluczowe obszary funkcjonalne:

\begin{itemize}
    \item \textbf{Sterowanie PTZ (\textit{Uplink}):} Zdarzenia płynące od użytkownika do serwera, sterujące silnikami kamery.
    \item \textbf{Powiadomienia o Alarmach (\textit{Downlink}):} Zdarzenia płynące z serwera do użytkownika, informujące o wykryciu ruchu przez algorytm analizy obrazu.
    \item \textbf{Zarządzanie Stanem Nagrywania:} Synchronizacja interfejsu użytkownika (np. zmiana koloru diody nagrywania) ze stanem procesu rejestracji wideo na serwerze.
\end{itemize}

\paragraph{Przykład implementacji: Sterowanie pozycją kamery}

Najbardziej obrazowym przykładem zastosowania EDA w projekcie jest mechanizm sterowania ruchem kamery (\textit{Pan/Tilt}). W tradycyjnym modelu HTTP, naciśnięcie przycisku musiałoby wysłać żądanie \texttt{POST}, a serwer musiałby odesłać odpowiedź, co przy szybkim klikaniu powodowałoby ,,zatykanie'' się kolejki żądań. W modelu zdarzeniowym proces ten przebiega asynchronicznie:

\begin{enumerate}
    \item \textbf{Produkcja (Klient):} Użytkownik naciska przycisk ,,W Lewo'' w interfejsie przeglądarki. Warstwa Prezentacji (JavaScript) przechwytuje zdarzenie \texttt{mousedown} i natychmiast emituje zdarzenie WebSocket o nazwie \texttt{'move\_camera'} z ładunkiem danych \texttt{\{'direction': 'left', 'step': 2\}}. Interfejs nie czeka na potwierdzenie, pozostając responsywnym.
    
    \item \textbf{Transport:} Zdarzenie jest przesyłane otwartym kanałem TCP do serwera, bez narzutu nagłówków HTTP.
    
    \item \textbf{Konsumpcja (Serwer):} Funkcja \texttt{handle\_move\_camera} w pliku \texttt{socket\_handlers.py}, która nasłuchuje na ten konkretny typ zdarzenia (\texttt{@socketio.on}), zostaje wybudzona. Przekazuje ona komendę do sterownika sprzętowego \texttt{camera.py}, który wykonuje fizyczny obrót urządzenia.
    
    \item \textbf{Sprzężenie zwrotne (Opcjonalne):} Jeśli kamera osiągnie fizyczny limit obrotu, warstwa logiki staje się nowym producentem zdarzenia. Emituje ona zdarzenie \texttt{'ptz\_limit'}, na które nasłuchuje przeglądarka, aby zablokować odpowiedni przycisk w interfejsie graficznym.
\end{enumerate}

Dzięki luźnemu powiązaniu komponentów (\textit{loose coupling}), serwer może obsłużyć setki takich zdarzeń na sekundę, zapewniając płynne sterowanie ,,oko-ręka'', niemożliwe do osiągnięcia w architekturze synchronicznej.

\subsection{Diagramy}
\subsection*{Diagram komponentow }
\subsection*{Diagram klas}
\subsection*{Diagram komunikacji}

\subsection{Zastosowane narzędzia i technologie}
\label{sec:narzedzia_i_technologie}

Niniejszy rozdział stanowi dogłębną analizę techniczną \textbf{stosu technologicznego} (\textit{technology stack}) dobranego do realizacji projektu inżynierskiego, którego celem jest stworzenie otwartego systemu obsługi kamer IoT, na przykładzie modelu TP-Link Tapo C200. Wybór poszczególnych komponentów nie był procesem arbitralnym, lecz wynikiem wieloaspektowej analizy wymagań funkcjonalnych i niefunkcjonalnych, ze szczególnym uwzględnieniem ograniczeń zasobowych urządzeń brzegowych (\textit{Edge Computing}), konieczności minimalizacji opóźnień (\textit{low-latency}) w przetwarzaniu strumienia audiowizualnego oraz imperatywu przełamania barier interoperacyjności narzuconych przez producenta (\textit{vendor lock-in}).

W poniższych podrozdziałach dokonano dekonstrukcji architektury systemu na poziomie narzędziowym, omawiając zarówno warstwę językową, środowiskową, jak i biblioteki specyficzne dla domeny przetwarzania sygnałów. Każda decyzja projektowa została osadzona w kontekście aktualnego stanu wiedzy (\textit{State of the Art}), odnosząc się do literatury przedmiotu w zakresie strumieniowania wideo, inżynierii wstecznej oraz optymalizacji systemów wbudowanych.

\subsubsection{Język Programowania}
\label{subsubsec:python_3_13}

Wybór języka \textbf{Python} w wersji \textbf{3.13} jako fundamentu warstwy logicznej projektu stanowił decyzję strategiczną, wynikającą z analizy wymagań stawianych współczesnym systemom IoT oraz aplikacjom przetwarzającym multimedia. W kontekście inżynierii oprogramowania, dobór technologii musi uwzględniać wypadkową dostępnych narzędzi oraz skalowalności rozwiązania. Python, dzięki swojemu dojrzałemu ekosystemowi, pełni w projektowanym systemie rolę \textbf{warstwy orkiestracji} (ang. \textit{glue code}).

\subsubsection*{Bogactwo ekosystemu bibliotecznego i interoperacyjność}
\label{par:python_ekosystem}

Kluczowym argumentem przemawiającym za wyborem tego środowiska jest dostępność i stabilność zaawansowanych bibliotek dedykowanych przetwarzaniu sygnałów. W ekosystemie Pythona możliwe jest wykorzystanie gotowych, wysoce zoptymalizowanych \textit{wrapperów} na biblioteki natywne. Moduły takie jak \texttt{threading} oraz \texttt{multiprocessing} pozwalają na efektywne zarządzanie operacjami wejścia/wyjścia (\textit{I/O bound}), co jest krytyczne dla zachowania płynności strumieniowania w czasie rzeczywistym.

\subsubsection*{Szybkie prototypowanie i paradygmat Rapid Application Development (RAD)}
\label{par:python_rad}

Specyfika pracy inżynierskiej wymaga narzędzi umożliwiających szybką iterację i weryfikację hipotez. Python, jako język dynamicznie typowany o wysokiej ekspresywności składni, drastycznie skraca cykl wytwórczy oprogramowania. W kontekście integracji z urządzeniami IoT, pozwala to na elastyczne dostosowywanie protokołów komunikacyjnych i logiki sterowania bez konieczności długotrwałej rekompilacji całego projektu.

\subsubsection{Zarządzanie Zależnościami}
\label{subsubsec:dependency_management}

W inżynierii oprogramowania systemów wbudowanych, \textbf{stabilność i powtarzalność środowiska} są kluczowe. Tradycyjne narzędzia zarządzania pakietami w Pythonie, takie jak \texttt{pip}, często zawodzą w złożonych scenariuszach CI/CD ze względu na wolny proces rozwiązywania zależności (\textit{dependency resolution}) i brak determinizmu.

\subsubsection*{Nowoczesne Narzędzie: \texttt{uv}}
\label{par:uv_tool}

W projekcie zastosowano \textbf{\texttt{uv}} – nowoczesny menedżer pakietów napisany w języku \textbf{Rust}. Narzędzie zostało wybrane ze względu na swoją bezkompromisową \textbf{wydajność}. Benchmarki wskazują, że \texttt{uv} potrafi instalować pakiety i rozwiązywać drzewa zależności od 10 do 100 razy szybciej niż standardowy \texttt{pip}. Skrócenie tego czasu znacząco przyspiesza cykl deweloperski (\textit{feedback loop}) w kontekście budowania obrazów \textbf{Docker}.

\subsubsection*{Determinizm i Pliki Blokady}
\label{par:uv_lockfiles}

Kluczowym aspektem dla pracy inżynierskiej jest gwarancja, że system wdrożony na urządzeniu produkcyjnym będzie posiadał identyczne wersje bibliotek co środowisko deweloperskie. \texttt{uv} wprowadza obsługę uniwersalnych \textbf{plików blokady} (\texttt{uv.lock}), które precyzyjnie definiują całe drzewo zależności wraz z \textbf{sumami kontrolnymi} (\textit{hashes}), gwarantując kryptograficzną spójność środowiska. Jest to mechanizm podnoszący standard inżynieryjny projektu, analogiczny do \texttt{Cargo.lock} w Rust.

\subsubsection*{Zaawansowana integracja z Dockerem}
\label{par:uv_docker_integration}

W projekcie wykorzystano specyficzne techniki optymalizacji współpracy \texttt{uv} z systemem plików Docker (\textit{OverlayFS}). Zastosowanie mechanizmu montowania \textit{cache'u} (\textit{BuildKit cache mounts}) pozwala na \textbf{persystencję pobranych artefaktów} pomiędzy kolejnymi budowaniami kontenera. Dodatkowo, strategia \textbf{Bytecode Compilation} wspierana natywnie przez \texttt{uv} skraca czas startu aplikacji (\textit{cold start}), co jest istotne w przypadku restartu usługi na urządzeniu monitoringu.

\subsubsection{Ekosystem Konteneryzacji}
\label{subsubsec:docker_iot_final}

Wdrożenie oprogramowania na \textbf{urządzeniach brzegowych} (\textit{Edge Devices}) wiąże się z wyzwaniami heterogeniczności sprzętowej i konfliktów bibliotecznych. Zastosowanie technologii \textbf{Docker} w niniejszym projekcie nie jest jedynie wygodą, lecz koniecznością architektoniczną zapewniającą \textbf{izolację}, \textbf{przenośność} i \textbf{bezpieczeństwo}.

\subsubsection*{Izolacja Procesów i Bezpieczeństwo}
\label{par:docker_bezpieczenstwo_final}

Kamery IoT, w tym modele \textbf{Tapo}, operują w strefie podwyższonego ryzyka cybernetycznego (patrz: analiza podatności \textit{CVE} w Rozdziale 2). Uruchomienie autorskiego serwera sterującego bezpośrednio na systemie operacyjnym hosta (\textit{bare-metal}) niosłoby ryzyko, że ewentualne przejęcie kontroli nad aplikacją dałoby atakującemu dostęp do całego systemu. Docker zapewnia silną \textbf{izolację procesów} wykorzystując mechanizmy jądra \textbf{Linux} (\textit{cgroups}, \textit{namespaces}).

W projekcie zastosowano dodatkowo praktykę \enquote{non-root user} wewnątrz kontenera oraz \textbf{minimalizację uprawnień} (\textit{capabilities drop}), co drastycznie redukuje powierzchnię ataku i chroni system hosta.

\subsubsection*{Multi-stage Builds i Optymalizacja Rozmiaru}
\label{par:multi_stage_builds_final}

Urządzenia klasy \textit{embedded} często dysponują ograniczoną przestrzenią dyskową. Aby pogodzić wymagania posiadania ciężkich narzędzi kompilacji (np. GCC, \texttt{numpy}) z koniecznością lekkiego obrazu końcowego, zastosowano technikę \textbf{budowania wieloetapowego} (\textit{Multi-stage Builds}). 

\begin{enumerate}
    \item \textbf{Stage 1 (Builder):} Obraz zawierający pełny \textit{toolchain} (kompilatory GCC, nagłówki systemowe, \texttt{uv}, \texttt{git}).
    \item \textbf{Stage 2 (Runtime):} Obraz typu \enquote{slim} (np. \texttt{python:3.13-slim-bookworm}), do którego kopiowane są jedynie wynikowe artefakty z etapu pierwszego.
\end{enumerate}
Dzięki temu podejściu, finalny obraz kontenera jest pozbawiony zbędnych plików tymczasowych, \textit{cache'u} i narzędzi deweloperskich, osiągając rozmiar rzędu \textbf{200-300 MB} zamiast ponad 1 GB, co przyspiesza jego dystrybucję i aktualizację.

\subsubsection{Interfejs Webowy i Protokół Komunikacji}
\label{subsubsec:web_interface_final}

Efektywna interakcja użytkownika z systemem IoT wymaga \textbf{warstwy prezentacji}, która jest w stanie obsłużyć dynamiczny charakter \textbf{danych strumieniowych}. W tradycyjnym modelu webowym, opartym na \textbf{bezstanowym protokole HTTP} (\textit{Request-Response}), realizacja płynnego sterowania w czasie rzeczywistym jest nieefektywna ze względu na narzut sieciowy (\textit{overhead}).

\subsubsection*{Serwer Aplikacyjny: Flask}
\label{par:flask_server_final}

Wybrano \textbf{Flask} – \textbf{lekki mikro-framework} w Pythonie (zgodny ze standardem \textit{WSGI}). W przeciwieństwie do rozwiązań typu \enquote{full-stack}, Flask nie narzuca sztywnej struktury. Posiada minimalny narzut pamięciowy i pełni rolę \enquote{cienkiego klienta} serwerowego, odpowiedzialnego za:
\begin{itemize}
    \item \textbf{Orkiestrację wątków:} Integracja asynchronicznych bibliotek sterujących kamerą.
    \item \textbf{Routing:} Obsługa statycznych plików interfejsu oraz końcówek API (\textit{endpoints}).
\end{itemize}

\subsubsection*{Protokół Transportowy: WebSockets}
\label{par:websockets_final}

Zastosowano protokół \textbf{WebSocket} (\textit{RFC 6455}) przy użyciu biblioteki \texttt{Flask-SocketIO}. Zapewnia on zestawienie \textbf{trwałego, dwukierunkowego kanału komunikacji} (pełny dupleks) między przeglądarką klienta a serwerem, eliminując opóźnienia wynikające z cyklicznego odpytywania (\textit{polling}). 

Umożliwia to realizację dwóch celów:
\begin{itemize}
    \item \textbf{Transmisja Wideo (\textit{Low-Latency Streaming}):} Klatki wideo są przesyłane jako \textbf{binarne ładunki} (\textit{binary payloads}) przez otwarty socket, co pozwala na redukcję opóźnień transmisji (\textit{latency}).
    \item \textbf{Sterowanie Czasu Rzeczywistego (\textit{Real-Time Control}):} Komendy sterujące \textbf{PTZ} (Pan/Tilt/Zoom) są przesyłane jako lekkie obiekty \textbf{JSON}. Czas reakcji kamery jest zminimalizowany ($<100ms$).
\end{itemize}

\subsubsection*{Warstwa Klienta (Frontend): Vanilla HTML/JS}
\label{par:frontend_final}

W warstwie interfejsu użytkownika podjęto świadomą decyzję o rezygnacji z rozbudowanych frameworków JavaScript (np. React, Vue) na rzecz \textbf{natywnych technologii webowych}: \textbf{Vanilla JavaScript}, HTML5 oraz CSS3. Zastosowanie \textbf{czystego JavaScriptu} pozwoliło na:
\begin{itemize}
    \item \textbf{Maksymalną wydajność renderowania:} Bezpośrednia manipulacja \textbf{drzewem DOM} (\textit{Document Object Model}) jest szybsza niż mechanizmy wirtualnego DOM.
    \item \textbf{Redukcję długu technologicznego:} Kod klienta nie wymaga procesu kompilacji (\textit{transpilacji/bundlingu}).
    \item \textbf{Intuicyjną obsługę:} Natywne \texttt{EventListeners} służą do przechwytywania zdarzeń klawiatury (sterowanie kamerą za pomocą strzałek).
\end{itemize}

\subsubsection{Biblioteki Przetwarzania Multimediów}
\label{subsubsec:biblioteki_multimedialne_final}

W projektowanym systemie nadzoru wizyjnego, kluczową rolę technologiczną odgrywa biblioteka \textbf{OpenCV} (\textit{Open Source Computer Vision Library}). Stanowi ona rdzeń analityczny, odpowiadając za \textbf{akwizycję strumienia wideo} z kamer TP-Link Tapo oraz jego zaawansowaną \textbf{analizę w czasie rzeczywistym}. Biblioteka \textbf{PyAV} została wprowadzona jako rozwiązanie komplementarne, dedykowane wyłącznie do obsługi \textbf{ścieżki dźwiękowej}.

\subsubsection*{OpenCV jako główny silnik wideo i analityczny}
\label{par:opencv_analiza_final}

Decyzja o uczynieniu OpenCV główną biblioteką projektu podyktowana była jej pozycją jako \textbf{standardu przemysłowego} oraz kompleksowością oferowanych rozwiązań. W ramach opracowanego oprogramowania, OpenCV realizuje pełen cykl życia danych wizyjnych: 

\begin{itemize}
    \item \textbf{Akwizycja Obrazu (\textit{Video Acquisition}):} Wykorzystanie interfejsu \texttt{cv2.VideoCapture} pozwala na \textbf{stabilne nawiązanie połączenia} ze strumieniem \textbf{RTSP} kamery.
    \item \textbf{Przetwarzanie Macierzowe i Analityka:} Po pobraniu klatki, OpenCV wykonuje na niej operacje \enquote{inteligentne} (\textit{Smart Features}). Zaimplementowany \textbf{algorytm detekcji ruchu} (oparty na odejmowaniu tła i filtracji Gaussa) oraz nanoszenie metadanych (\textit{OSD}) są realizowane bezpośrednio na obiektach tej biblioteki.
    \item \textbf{Optymalizacja:} Dzięki \textbf{backendowi napisanemu w C++}, OpenCV zapewnia wysoką wydajność operacji na macierzach, co jest kluczowe przy przetwarzaniu obrazu o wysokiej rozdzielczości na urządzeniach o ograniczonej mocy obliczeniowej.
\end{itemize}

\subsubsection*{PyAV: Uzupełnienie luki funkcjonalnej (Audio)}
\label{par:pyav_audio_final}

Mimo wszechstronności w dziedzinie wideo, \textbf{OpenCV} posiada ograniczenia w zakresie \textbf{obsługi dźwięku} – biblioteka ta całkowicie ignoruje pakiety audio przesyłane w kontenerze RTSP.

W celu rozwiązania tego problemu inżynierskiego zastosowano bibliotekę \textbf{PyAV} (\textit{binding} dla \textbf{FFmpeg}). Jej rola w projekcie jest ściśle zdefiniowana i ograniczona do: \textbf{Równoległego nawiązania połączenia}, \textbf{Ekstrakcji, dekodowania i transkodowania strumienia audio} (z formatów PCM/AAC), przy jednoczesnym ignorowaniu pakietów wideo w celu oszczędności zasobów CPU.

Taka architektura pozwala na wykorzystanie pełnej mocy OpenCV do analizy obrazu, delegując jedynie niezbędne minimum (obsługę mikrofonu) do wyspecjalizowanej biblioteki PyAV.

\begin{table}[H]
    \centering
    \caption{Podział kompetencji w warstwie multimedialnej}
    \label{tab:podzial_multimedia_final}
    \begin{tabularx}{\textwidth}{L L L}
        \toprule
        \textbf{Biblioteka} & \textbf{Status w projekcie} & \textbf{Odpowiedzialność} \\
        \midrule
        OpenCV & Główna (\textit{Core}) & Pobieranie wideo (RTSP), dekodowanie obrazu, detekcja ruchu, nanoszenie OSD, przygotowanie klatek do streamingu. \\
        \midrule
        PyAV & Pomocnicza (\textit{Auxiliary}) & Przechwytywanie wyłącznie ścieżki audio, transkodowanie dźwięku. \\
        \bottomrule
    \end{tabularx}
\end{table}


\subsubsection{Kontrola Kamery i Inżynieria Wsteczna}
\label{subsubsec:pytapo_reverse_engineering_final}

Realizacja nadrzędnego celu pracy – \textbf{pełnego uniezależnienia systemu monitoringu od infrastruktury chmurowej producenta} – wymagała rozwiązania problemu \textbf{zamkniętej architektury} urządzenia. Kamera TP-Link Tapo C200 nie udostępnia publicznego \textbf{API} dla sieci lokalnej (\textit{LAN}), co jest klasycznym przykładem strategii \textbf{\textit{Vendor Lock-in}}.

Aby przełamać to ograniczenie, w warstwie sterowania wykorzystano bibliotekę \textbf{PyTapo}. Jest to rozwiązanie typu \textbf{Open Source}, stanowiące implementację klienta własnościowego protokołu komunikacyjnego TP-Link, powstałe w wyniku procesów \textbf{inżynierii wstecznej} (\textit{Reverse Engineering}).

\subsubsection*{Mechanizm działania i emulacja klienta}
\label{par:pytapo_emulacja_final}

Działanie biblioteki opiera się na \textbf{symulacji zachowania oficjalnej aplikacji mobilnej}. Analiza ruchu sieciowego wykazała, że kamera wykorzystuje zmodyfikowany protokół \textbf{HTTP} do przesyłania sterujących ładunków danych (\textit{payloads}) w formacie \textbf{JSON}. Komunikacja ta jest zabezpieczona na kilku poziomach, które \textbf{PyTapo} skutecznie emuluje:

\begin{itemize}
    \item \textbf{Negocjacja sesji (\textit{Handshake}):} Biblioteka implementuje złożony proces \textbf{uwierzytelniania}, wymagający wymiany \textbf{kluczy sesyjnych} oraz \textbf{tokenów} (\textit{stok}), generowanych w oparciu o algorytmy skrótu (MD5/SHA) i liczby losowe (\textit{nonce}).
    \item \textbf{Szyfrowanie Payloadu:} W przeciwieństwie do otwartych standardów, parametry sterujące (np. koordynaty silnika PTZ) nie są przesyłane jawnym tekstem. PyTapo implementuje algorytmy \textbf{szyfrowania symetrycznego} (warianty AES), co pozwala na konstruowanie poprawnych, zaszyfrowanych zapytań.
\end{itemize}

\subsubsection*{Przewaga nad standardem ONVIF}
\label{par:pytapo_vs_onvif_final}

Wybór PyTapo był podyktowany ograniczeniami implementacyjnymi standardu \textbf{ONVIF} (\textit{Open Network Video Interface Forum}), który w tanich kamerach Tapo ogranicza się często tylko do strumieniowania wideo (\textit{RTSP}).

Zastosowanie PyTapo umożliwiło dostęp do \enquote{ukrytych} \textbf{funkcji administracyjnych}, niedostępnych przez generyczne sterowniki:

\begin{itemize}
    \item \textbf{Pełna kontrola PTZ} (\textit{Pan-Tilt-Zoom}): Precyzyjne sterowanie silnikami krokowymi kamery.
    \item \textbf{Zarządzanie sensorem:} Programowe przełączanie trybu nocnego (\textbf{kontrola filtra IR-Cut}) oraz regulacja czułości detekcji ruchu.
    \item \textbf{Funkcje prywatności:} Możliwość zdalnego \textbf{wygaszenia obiektywu} (\textit{Privacy Mode}) lub wyłączenia diody statusu LED.
    \item \textbf{Formatowanie nośników:} Zdalne zarządzanie kartą SD.
\end{itemize}

\subsubsection{Narzędzie do Kompozycji i Zapisu Danych}
\label{subsubsec:moviepy_kompozycja_final}

Ostatnim ogniwem w łańcuchu przetwarzania danych multimedialnych jest moduł odpowiedzialny za \textbf{trwały zapis} (\textit{persystencję}) materiału dowodowego. Ze względu na przyjętą \textbf{architekturę hybrydową}, w której obraz i dźwięk przetwarzane są przez niezależne biblioteki (\textbf{OpenCV} i \textbf{PyAV}), zaistniała konieczność zastosowania narzędzia efektywnie integrującego te dwa rozłączne strumienie. Do realizacji tego zadania wybrano bibliotekę \textbf{MoviePy}.

\subsubsection*{Rola integratora strumieni (Multipleksing)}
\label{par:moviepy_muxing_final}

MoviePy pełni w projekcie funkcję \textbf{orkiestratora procesu zapisu}. Jego zadaniem jest przeprowadzenie \textbf{multipleksowania} (\textit{muxing}), czyli scalenia danych wizyjnych (\textbf{macierzy NumPy} z OpenCV) i danych fonicznych (\textbf{próbek audio} z PyAV) zgromadzonych w buforach pamięci. Wynikiem jest \textbf{enkapsulacja} do standardowego \textbf{kontenera multimedialnego} (\texttt{MP4}) z kodekami \textbf{H.264} i \textbf{AAC}. Wybór dedykowanej biblioteki gwarantuje zachowanie \textbf{spójności struktury pliku wynikowego}.

\subsubsection*{Abstrakcja nad FFmpeg i Synchronizacja A/V}
\label{par:moviepy_synchronizacja_final}

MoviePy to wysokopoziomowa nakładka (\textit{wrapper}) na oprogramowanie \textbf{FFmpeg}. Zastosowanie jej eliminuje złożoność bezpośredniego wywoływania komend FFmpeg i zapewnia automatyczną \textbf{synchronizację A/V} (\textit{lip-sync}), zarządzając osią czasu i dopasowując długość ścieżki audio do sekwencji wideo. To kluczowe w przypadku \textbf{detekcji ruchu}, gdzie nagrania mają zmienną długość. Proces kodowania odbywa się w sposób \textbf{wsadowy} (\textit{batch processing}) w momencie zakończenia nagrania.

\subsubsection*{Rozszerzalność (Extensibility)}
\label{par:moviepy_rozszerzalnosc_final}

Biblioteka ta oferuje bogaty zestaw funkcji do \textbf{nieliniowego montażu wideo} (\textit{NLE}) z poziomu kodu, co ułatwia przyszły rozwój oprogramowania i implementację dodatkowych funkcjonalności, takich jak:

\begin{itemize}
    \item \textbf{Dynamiczne dodawanie znaków wodnych} (\textit{Watermarking}).
    \item \textbf{Łączenie (konkatenacja)} wielu klipów zdarzeń w jeden raport wideo.
    \item \textbf{Nakładanie napisów końcowych} z parametrami zdarzenia (data, typ wykrytego obiektu).
\end{itemize}

\subsection{Proces implementacji rozwiazania}
\label{subsec:proces_implementacji}
\subsubsection{Serwer http}
\subsubsection{Implementacja połączenia z kamerą}
\subsubsection{Client}
\subsubsection{API}
\subsubsection{Przechwytywanie audio}
\subsubsection{Przechwytywanie wideo}
\subsubsection{Sterowanie kamerą - PTZ}
\subsubsection{Algorytm wykrywania ruchu}
\subsubsection{Nagrywanie}
\subsubsection{Zapis}

\subsection{Podsumowanie}
\label{subsec:podsumowanie_roz3}